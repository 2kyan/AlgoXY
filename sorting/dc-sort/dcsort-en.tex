\ifx\wholebook\relax \else
\documentclass[b5paper]{article}
\usepackage[nomarginpar
  %, margin=.5in
]{geometry}

\addtolength{\oddsidemargin}{-0.05in}
\addtolength{\evensidemargin}{-0.05in}
\addtolength{\textwidth}{0.1in}
\usepackage[en]{../../prelude}

\setcounter{page}{1}

\begin{document}

\title{Quick sort and merge sort}

\author{Xinyu~LIU
\thanks{{\bfseries Xinyu LIU} \newline
  Email: liuxinyu95@gmail.com \newline}
  }

\maketitle
\fi

\markboth{Quick sort and merge sort}{Elementary Algorithms}

\ifx\wholebook\relax
\chapter{Quick sort and merge sort}
\numberwithin{Exercise}{chapter}
\fi

\section{Introduction}
\label{introduction}

People proved the performance upper limit be $O(n \lg n)$ for comparison based sort\cite{TAOCP}. This chapter gives two divide and conquer sort algorithms: quick sort and merge sort, both achieve $O(n \lg n)$ time bound. We also give their variants, like natural merge sort, in-place merge sort, and etc.

\section{Quick sort}
\index{Quick sort}

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.3]{img/kids}
 \captionsetup{labelformat = empty}
 \label{fig:knuth-ssort}
\end{figure}

Consider arrange kids in a line ordered by height.

\begin{enumerate}
\item The first kid raises hand, all shorter one move to left, and the others move to right;
\item All kids on the left and right repeat.
\end{enumerate}

For example, the heights (in cm) are $[102, 100, 98, 95, 96, 99, 101, 97]$. Table \ref{tab:kids-sort} gives the steps. (1) The kid of 102 cm raises hand as the pivot (underlined in the first row). It happens the tallest, hence all others move to the left as shown in the second row in the table. (2) The kid of 100 cm is the pivot. Kids of height 98, 95, 96, and 99 cm move to the left, and the kid of 101 cm move to the right, as shown in the third row. (3) The kid of 98 cm is the left pivot, while 101 cm is the right pivot. Because there is only one kid on the right, it's sorted. Repeat this to sort all kids.

\begin{table}[htbp]
\centering
\begin{tabular}{ | c c c c c c c c |}
\hline
\underline{102} & 100 & 98 & 95 & 96 & 99 & 101 & 97 \\
\underline{100} & 98 & 95 & 96 & 99 & 101 & 97 & `102' \\
\underline{98} & 95 & 96 & 99 & 97 & `100' & 101 & `102' \\
\underline{95} & 96 & 97 & `98' & 99 & `100' & `101' & `102' \\
`95' & \underline{96} & 97 & `98' & `99' & `100' & `101' & `102' \\
`95' & `96' & 97 & `98' & `99' & `100' & `101' & `102' \\
`95' & `96' & `97' & `98' & `99' & `100' & `101' & `102' \\
\hline
\end{tabular}
\caption{Sort steps}
\label{tab:kids-sort}
\end{table}

We can summarize the quick sort definition, when sort list $L$:

\begin{itemize}
\item If $L$ is empty$[\ ]$, the result is $[\ ]$;
\item Otherwise, select an element as the pivot $p$, recursively sort elements $\leq p$ to the left; {\em and} sort other elements $> p$ to the right.
\end{itemize}

We say {\em and}, but not `then', indicate we can parallel sort left and right. C. A. R. Hoare developed quick sort in 1960\cite{TAOCP}\cite{wiki-qs}. There are varies of ways to pick the pivot, for example, always choose the first element.

\be
\begin{array}{rcl}
sort\ [\ ] & = & [\ ] \\
sort\ (x \cons xs) & = & sort\ [y | y \in xs, y \leq x] \doubleplus [x] \doubleplus sort\ [y | y \in xs, x < y] \\
\end{array}
\ee

We use the Zermelo Frankel expression (ZF expression)\footnote{Name after two mathematicians found the modern set theory.}. $\{ a | a \in S, p_1(a), p_2(a), ... \}$ selects elements in set $S$, that satisfy every the predication $p_1, p_2, ...$ (see chapter 1). Below is example code:

\lstset{frame = single}
\begin{Haskell}
sort [] = []
sort (x:xs) = sort [y | y<-xs, y <= x] ++ [x] ++ sort [y | y<-xs, x < y]
\end{Haskell}

We assume to sort in ascending order. We can abstract the comparison to sort different things like numbers, strings, and etc. (see chapter 3) We needn't total ordering, but at least need {\em strict weak ordering}\cite{wiki-total-order}\cite{wiki-sweak-order}(see chapter 9). We use $\leq$ as the abstract comparison.

\subsection{Partition}
\index{Quick sort!partition}
We traverse elements in two passes: first filter all elements $\leq x$ ; next filter all $> x$. We can combine them into one pass:

\be
\begin{array}{rcl}
\textit{part}\ p\ [\ ] & = & ([\ ], [\ ]) \\
\textit{part}\ p\ (x \cons xs) & = & \begin{cases}
 p(x): & (x \cons as, bs), \text{where}: (as, bs) = \textit{part}\ p\ xs \\
 \text{otherwise}: & (as, x \cons bs) \\
\end{cases} \\
\end{array}
\ee

And change the quick sort definition to:

\be
\begin{array}{rcl}
sort\ [\ ] & = & [\ ] \\
sort\ (x \cons xs) & = & sort\ as \doubleplus [x] \doubleplus sort\ bs, \text{where}: (as, bs) = \textit{part}\ (\leq x)\ xs \\
\end{array}
\ee

We can also define partition with fold:

\be
\textit{part}\ p\ = foldr\ f\ ([\ ], [\ ])
\ee

Where $f$ is defined as:

\be
f\ (as, bs)\ x = \begin{cases}
p(x): & (x \cons as, bs) \\
\text{otherwise}: & (as, x \cons bs) \\
\end{cases}
\ee

It's essentially to accumulate to $(as, bs)$. If $p(x)$ holds, then add $x$ to $as$, otherwise to $bs$. We can implement a tail recursive partition:

\be
\begin{array}{rcl}
\textit{part}\ p\ [\ ]\ as\ bs & = & (as, bs) \\
\textit{part}\ p\ (x \cons xs)\ as\ bs & = & \begin{cases}
  p(x): & \textit{part}\ p\ xs\ (x \cons as)\ bs \\
  \text{otherwise}: & \textit{part}\ p\ xs\ as\ (x \cons bs) \\
\end{cases}
\end{array}
\ee

To partition $x \cons xs$, we call:

\[
(as, bs) = \textit{part}\ (\leq x)\ xs\ [\ ]\ [\ ]
\]

We change concatenation $sort\ as \doubleplus [x] \doubleplus sort\ bs$ with accumulator as:

\be
\begin{array}{rcl}
sort\ s\ [\ ] & = & s \\
sort\ s\ (x \cons xs) & = & sort\ (x : sort\ s\ bs)\ as \\
\end{array}
\ee

Where $s$ is the accumulator, we initialize sort with an empty list: $qsort = sort\ [\ ]$. After partition, we need recursively sort $as, bs$. We can first sort $bs$, prepend $x$, then pass it as the new accumulator to sort $as$:

\begin{Haskell}
sort = sort' []

sort' acc [] = acc
sort' acc (x:xs) = sort' (x : sort' acc bs) as where
  (as, bs) = part xs [] []
  part [] as bs = (as, bs)
  part (y:ys) as bs | y <= x = part ys (y:as) bs
                    | otherwise = part ys as (y:bs)
\end{Haskell}

\subsection{In-place sort}

Figure \ref{fig:partition-1-way} gives a way to partition in-place\cite{Bentley}\cite{CLRS}. We scan from left to right. At any time, the array is consist of three parts as shown in figure \ref{fig:partition-1-way} (a):

\begin{figure}[htbp]
   \centering
   \subcaptionbox{Partition invariant}{
      \begin{tikzpicture}[scale=0.8]
      \draw (0, 0) rectangle (1, 1) node (xl) [pos=.5] {x[l]}
            (1, 0) rectangle (5, 1) node (leq) [pos=.5] {... $\leq p$ ...}
            (5, 0) rectangle (9, 1) node (ge) [pos=.5] {... $> p$ ...}
            (9, 0) rectangle (11, 1) node (rest) [pos=.5] {...?...}
            (11, 0) rectangle (12, 1) node (xu) [pos=.5] {x[u]};
      \fill [black] (5, 0) rectangle (5.1, 1) node (leftbar) [pos=.5] {}
                    (9, 0) rectangle (9.1, 1) node (rightbar) [pos=.5] {};
      \draw (0, 2) node (pivot) {$p = x[l]$}
            (5, 2) node (left) {left $L$}
            (9, 2) node (right) {right $R$};
      \draw[thick, ->] (pivot) edge (xl)
                       (left) edge [bend right] (leftbar)
                       (right) edge [bend right] (rightbar);
      \end{tikzpicture}} \\
   \subcaptionbox{Initialize}{
      \begin{tikzpicture}[scale=0.8]
      \draw (-0.5, 0) rectangle (1, 1) node (xl) [pos=.5] {x[l]}
            (1, 0) rectangle (2.5, 1) node (xl1) [pos=.5] {x[l+1]}
            (2.5, 0) rectangle (4, 1) node (rest) [pos=.5] (ai) {...?...}
            (4, 0) rectangle (5, 1) node (xu) [pos=.5] {x[u]};
      \fill [black] (1, 0) rectangle (1.1, 1) node (leftbar) [pos=.5] {};
      \draw (-2, 2) node (pivot) {$p$}
            (0, 2) node (left) {$L$}
            (2, 2) node (right) {$R$};
      \draw[thick, ->] (pivot) edge [bend right] (xl)
                       (left) edge [bend right] (leftbar)
                       (right) edge [bend left] (leftbar);
      \end{tikzpicture}} \\
   \subcaptionbox{Terminate}{
      \begin{tikzpicture}[scale=0.8]
      \draw (0, 0) rectangle (1, 1) node (xl) [pos=.5] {x[l]}
            (1, 0) rectangle (5, 1) node (leq) [pos=.5] {... $\leq p$ ...}
            (5, 0) rectangle (6, 1) node (xleft) [pos=.5] {x[$L$] }
            (6, 0) rectangle (10, 1) node (ge) [pos=.5] {... $> p$ ...}
            (10, 0) rectangle (11, 1) node (xu) [pos=.5] {x[u]};
      \fill [black] (6, 0) rectangle (6.1, 1) node (leftbar) [pos=.5] {}
                    (11, 0) rectangle (11.1, 1) node (rightbar) [pos=.5] {};
      \draw (0, 2) node (pivot) {$p$}
            (6, 2) node (left) {$L$}
            (12, 2) node (right) {$R$};
      \draw[thick, ->] (pivot) edge (xl)
                       (left) edge [bend right] (leftbar)
                       (right) edge [bend left] (rightbar);
      \draw[thick, <->] (xl) edge [bend right] node [below] {swap} (xleft);
      \end{tikzpicture}} \\
   \caption{In-place partition, pivot $p = x[l]$}
   \label{fig:partition-1-way}
\end{figure}

\begin{itemize}
\item The pivot is the left element $p = x[l]$. It moves to the final position after partition;
\item A section of elements $\leq p$, extend right to $L$;
\item A section of elements $> p$, extend right to $R$. The elements between $L$ and $R$ $> p$;
\item Elements after $R$ haven't been partitioned (may $>, =, <$ p).
\end{itemize}

When partition starts, $L$ points to $p$, $R$ points to the next, as shown in figure \ref{fig:partition-1-way} (b). We advance $R$ to right till reach to the array boundary. Every time, we compare $x[R]$ and $p$. If $x[R] > p$, it should be between $L$ and $R$, we move $R$ forward; otherwise if $X[R] \leq p$, it should be on the left of $L$. We advance $L$ a step, then swap $x[L] \leftrightarrow x[R]$. When $R$ passes the last element, the partition ends. Elements $> p$ move to the right of $L$, while others on the left side. We need move $p$ to the position between the two parts. To do that, we swap $p \leftrightarrow x[L]$, as shown in \ref{fig:partition-1-way} (c). $L$ finally points to $p$, partitioned the array in two parts. We return $L + 1$ as the result, that points to the first element $> p$. Let the array be $A$, the lower, upper boundary be $l, u$. The in-place partition is defined below:

\begin{algorithmic}[1]
\Function{Partition}{A, l, u}
  \State $p \gets A[l]$  \Comment{pivot}
  \State $L \gets l$ \Comment{left}
  \For{$R$ in $[l+1, u]$} \Comment{iterate right}
    \If{$p \geq A[R]$}
      \State $L \gets L + 1$
      \State \textproc{Exchange} $A[L] \leftrightarrow A[R]$
    \EndIf
  \EndFor
  \State \textproc{Exchange} $A[L] \leftrightarrow p$
  \State \Return $L + 1$ \Comment{partition position}
\EndFunction
\end{algorithmic}

Table \ref{tab:partition-steps} lists the steps to partition $[3, 2, 5, 4, 0, 1, 6, 7]$.

\begin{table}[htbp]
\centering
\begin{tabular}{|llllllll|l|}
\hline
\underline{3}(l)  & 2(r) & 5 & 4 & 0 & 1 & 6 & 7 & start, $p = 3$、$l = 1$、$r = 2$ \\
\underline{3} & 2(l)(r) & 5 & 4 & 0 & 1 & 6 & 7 & $2 < 3$, advance $l$（$r=l$）\\
\underline{3} & 2(l) & 5(r) & 4 & 0 & 1 & 6 & 7 & $5 > 3$, move on \\
\underline{3} & 2(l) & 5 & 4(r) & 0 & 1 & 6 & 7 & $4 > 3$, move on \\
\underline{3} & 2(l) & 5 & 4 & 0(r) & 1 & 6 & 7 & $0 < 3$ \\
\underline{3} & 2 & 0(l) & 4 & 5(r) & 1 & 6 & 7 & advance $l$, swap with $r$ \\
\underline{3} & 2 & 0(l) & 4 & 5 & 1(r) & 6 & 7 & $1 < 3$ \\
\underline{3} & 2 & 0 & 1(l) & 5 & 4(r) & 6 & 7 & advance $l$, swap with $r$ \\
\underline{3} & 2 & 0 & 1(l) & 5 & 4 & 6(r) & 7 & $6 > 3$, move on \\
\underline{3} & 2 & 0 & 1(l) & 5 & 4 & 6 & 7(r) & $7 > 3$, move on \\
1 & 2 & 0 & 3 & 5(l+1) & 4 & 6 & 7 & terminate, swap $p$ and $l$ \\
\hline
\end{tabular}
\caption{Partition array} \label{tab:partition-steps}
\end{table}

With \textproc{Partition} defined, we implement quick sort as below:

\begin{algorithmic}[1]
\Procedure{Quick-Sort}{$A, l, u$}
  \If{$l < u$}
    \State $m \gets$ \Call{Partition}{$A, l, u$}
    \State \Call{Quick-Sort}{$A, l, m - 1$}
    \State \Call{Quick-Sort}{$A, m, u$}
  \EndIf
\EndProcedure
\end{algorithmic}

We pass the array and its boundaries, as \textproc{Quick-Sort}($A, 1, |A|$) to sort. When the array is empty or singleton, sort returns immediately.

\begin{Exercise}
\Question{Improve the basic quick sort definition when the list is singleton.}
\end{Exercise}

\subsection{Performance}
\index{Quick sort!Performance}

Quick sort performs well in most cases. We start from the best/worst cases. For the best case, we always halve the elements into two equal sized parts. As shown in figure \ref{fig:qsort-best}, there are total $O(\lg n)$ levels of recursions. At level one, we processes $n$ elements with one partition; at level two, we partition twice, each processes $n/2$ elements, taking total $2 O(n/2) = O(n)$ time; at level three, we partition four times, each process $n/4$ elements, taking total $O(n)$ time too, ..., at the last level, there are $n$ singleton segments, taking total $O(n)$ time. Sum all levels, the time is bound to $O(n \lg n)$.

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.55]{img/qsort-best}
 \caption{The best case, halve every time.}
 \label{fig:qsort-best}
\end{figure}

For the worst case, the partition is totally unbalanced, one part is of $O(1)$ length, the other is $O(n)$. The level of recursions decays to $O(n)$. Model the partition as a tree. It's balanced binary tree in the best case, while it becomes a linked-list of $O(n)$ length in the worst case. Every branch node has an empty sub-tree. At each level, we process all elements, hence the total time is bound to $O(n^2)$. This is same as insertion sort, and selection sort. We can list several worst cases, for example, there are many duplicated elements, or the sequence is largely ordered, and so on. There isn't a method can avoid the worst case completely.

\subsubsection{Average case\texorpdfstring{$\bigstar$}{★}}
\index{Quick Sort!Average case}

Quick sort performs well in average. For example, even if every partition gives two parts of 1:9, the performance still achieves $O(n \lg n)$\cite{CLRS}. We give two method to evaluate the performance. The first one is based on the fact, that the performance is proportion to the number of comparisons. In selection sort, every two elements are compared, while in quick sort, we save many comparisons. When partition sequence $[a_1, a_2, a_3, ..., a_n]$ with $a_1$ as the pivot, we obtain two sub sequences $A = [x_1, x_2, ..., x_k]$ and $B = [y_1, y_2, ..., y_{n-k-1}]$. After that, none element in $A$ will compare with any one in $B$. Let the sorted result be $[a_1, a_2, ..., a_n]$, if $a_i < a_j$, we do not compare them if and only if there is some element $a_k$, where $a_i < a_k < a_j$, is picked as the pivot before either $a_i$ or $a_j$ being the pivot. In other word, the only chance that we compare $a_i$ and $a_j$ is either $a_i$ or $a_j$ is chosen as the pivot before any other elements in $a_{i+1} < a_{i+2} < ... < a_{j-1}$ being the pivot. Let $P(i, j)$ be the probability that we compare $a_i$ and $a_j$. We have:

\be
P(i, j) = \frac{2}{j - i + 1}
\ee

The total number of comparisons is:

\be
C(n) = \sum_{i=1}^{n-1}\sum_{j=i+1}^{n} P(i, j)
\ee

If we compare $a_i$ and $a_j$, we won't compare $a_j$ and $a_i$ again, and we never compare $a_i$ with itself. The upper bound of $i$ is $n-1$, and the lower bound of $j$ is $i+1$. Substitute the probability:

\be
\begin{array}{rl}
C(n) & = \displaystyle \sum_{i=1}^{n-1}\sum_{j = i+1}^{n} \frac{2}{j - i + 1} \\
     & = \displaystyle \sum_{i=1}^{n-1}\sum_{k=1}^{n-i} \frac{2}{k+1} \\
\end{array}
\ee

Use the result of harmonic series\cite{wiki-harmonic}.

\[
H_n = 1 + \frac{1}{2} + \frac{1}{3} + .... = \ln n + \gamma + \epsilon_n
\]

\be
C(n) = \sum_{i=1}^{n-1} O(\lg n) = O(n \lg n)
\ee

The other method uses the recursion. Let the length of the sequence be $n$, we partition it into two parts of length $i$ and $n-i-1$. The partition takes $cn$ time because it compares every element with the pivot. The total time is:

\be
T(n) = T(i) + T(n-i-1) + c n
\ee

Where $T(n)$ is the time to sort $n$ elements. $i$ equally distributes across $0, 1, ..., n-1$. Taking math expectation:

\be
\renewcommand*{\arraystretch}{1.5}
\begin{array}{rl}
T(n) & = E(T(i)) + E(T(n-i-1)) + c n \\
     & = \displaystyle \frac{1}{n} \sum_{i=0}^{n-1}T(i) + \frac{1}{n} \sum_{i=0}^{n-1}T(n-i-1) + cn \\
     & = \displaystyle \frac{1}{n} \sum_{i=0}^{n-1}T(i) + \frac{1}{n} \sum_{j=0}^{n-1}T(j) + cn \\
     & = \displaystyle \frac{2}{n} \sum_{i=0}^{b-1}T(i) + cn
\end{array}
\ee

Multiply $n$ to both sides:

\be
n T(n) = 2 \sum_{i=0}^{n-1} T(i) + c n^2
\label{eq:ntn}
\ee

Substitute $n$ to $n-1$:

\be
(n-1) T(n-1) = 2 \sum_{i=0}^{n-2} T(i) + c (n-1)^2
\label{eq:n1tn1}
\ee

Take (\ref{eq:ntn}) - (\ref{eq:n1tn1}), cancel all $T(i)$ for $0 \leq i < n-1$.

\be
n T(n) = (n + 1) T(n-1) + 2cn - c
\ee

Drop the constant $c$, we obtain:

\be
\frac{T(n)}{n+1} = \frac{T(n-1)}{n} + \frac{2c}{n+1}
\ee

Assign $n$ to $n-1$, $n-2$, ..., to give $n-1$ equations.

\[
\frac{T(n-1)}{n} = \frac{T(n-2)}{n-1} + \frac{2c}{n}
\]

\[
\frac{T(n-2)}{n-1} = \frac{T(n-3)}{n-2} + \frac{2c}{n-1}
\]

\[
...
\]

\[
\frac{T(2)}{3} = \frac{T(1)}{2} + \frac{2c}{3}
\]

Sum up and cancel the same components on both sides, we get a function of $n$.

\be
\frac{T(n)}{n+1} = \frac{T(1)}{2} + 2c \sum_{k=3}^{n+1} \frac{1}{k}
\ee

Use the result of the harmonic series:

\be
O(\frac{T(n)}{n+1}) = O(\frac{T(1)}{2} + 2c \ln n + \gamma + \epsilon_n) = O(\lg n)
\ee

Therefore:

\be
O(T(n)) = O(n \lg n)
\ee

\subsection{Improvement}
\index{Quick sort!Improvement} \index{Quick sort!Ternary partition}

The \textproc{Partition} procedure doesn't perform well when there are many duplicated elements. Consider the extreme case that all $n$ elements are equal $[x, x, ..., x]$:

\begin{enumerate}
\item From the quick sort definition: pick any element as the pivot, hence $p = x$, partition into two sub-sequences. One is $[x, x, ..., x]$ of length $n - 1$, the other is empty. Next recursively sort the $n-1$ elements, the total time decays to $O(n^2)$.
\item Modify the partition with $< x$ and $> x$. The result are two empty sub-sequences, and $n$ elements equal to $x$. The recursion on empty sequence terminates immediately. The result is $[\ ] \doubleplus [x, x, ..., x] \doubleplus [\ ]$. The performance is $O(n)$.
\end{enumerate}

We improve from {\em binary} partition to {\em ternary} partition to handle duplicated elements:

\be
\begin{array}{rcl}
sort\ [\ ] & = & [\ ] \\
sort\ (x \cons xs) & = & sort\ S \doubleplus sort\ E \doubleplus sort\ G
\end{array}
\ee

Where:

\[
\begin{cases}
S = [ y | y \in xs, y < x ] \\
E = [ y | y \in xs, y = x ] \\
G = [ y | y \in xs, y > x ] \\
\end{cases}
\]

To concatenate three lists in linear time, we can use an accumulator: $qsort = sort\ [\ ]$, where:

\be
\begin{array}{rcl}
sort\ A\ [\ ] & = & A \\
sort\ A\ (x \cons xs) & = & sort\ (E \doubleplus sort\ A\ G)\ S \\
\end{array}
\ee

We partition the list in three parts: $S, E, G$, where $E$ contains elements of same value, hence sorted. We first sort $G$ with accumulator $A$, append the result to $E$ as the new accumulator, and use it to sort $S$. We also improve the partition with accumulator:

\be
\begin{array}{rcl}
part\ S\ E\ G\ x\ [\ ] & = & (S, E, G) \\
part\ S\ E\ G\ x\ (y \cons ys) & = & \begin{cases}
  y < x: & (y \cons S, E, G) \\
  y = x: & (S, y \cons E, G) \\
  y > x: & (S, E, y \cons G) \\
  \end{cases} \\
\end{array}
\ee

Richard Bird developed another improvement\cite{fp-pearls}, instead concatenate the
recursive sort results, put them in a list and concatenate finally:

\begin{Haskell}
sort :: (Ord a) => [a] -> [a]
sort = concat . (pass [])

pass xss [] = xss
pass xss (x:xs) = step xs [] [x] [] xss where
    step [] as bs cs xss = pass (bs : pass xss cs) as
    step (x':xs') as bs cs xss | x' <  x = step xs' (x':as) bs cs xss
                               | x' == x = step xs' as (x':bs) cs xss
                               | x' >  x = step xs' as bs (x':cs) xss
\end{Haskell}

\index{Quick Sort!2-way partition}
Robert Sedgewick developed two-way partition method\cite{qsort-impl}\cite{Bentley}. Use two pointers $i, j$ from left and right boundaries. Pick the first element as the pivot $p$. Advance $i$ to right till an element $\geq p$; while (in parallel) move $j$ to left till an element $\leq p$. At this time, all elements left to $i$ are less than the pivot ($< p$), while those right to $j$ are greater than the pivot ($> p$). $i$ points to one that $\geq p$, and $j$ points to one that $\leq p$, as shown in figure \ref{fig:partition-2-way} (a). To move all elements $\leq p$ to left, and the remaining to right, we exchange $x[i] \leftrightarrow x[j]$, then continue scan. We repeat this till $i$ and $j$ meet. At any time, we keep the invariant: All elements left to $i$ (include $i$) are $\leq p$; while all right to $j$ (include $j$) are $\geq p$. The elements between $i$ and $j$ are yet to scan, as shown in figure \ref{fig:partition-2-way} (b).

\begin{figure}[htbp]
   \centering
   \subcaptionbox{When $i$ and $j$ stop}{
      \begin{tikzpicture}[scale=0.8]
      \draw (0, 0) rectangle (1, 1) node (xl) [pos=.5] {x[l]}
            (1, 0) rectangle (5, 1) node (leq) [pos=.5] {... $< p$ ...}
            (5, 0) rectangle (6, 1) node (xi) [pos=.5] {x[i]}
            (6, 0) rectangle (8, 1) node (rest) [pos=.5] {... ? ...}
            (8, 0) rectangle (9, 1) node (xj) [pos=.5] {x[j]}
            (9, 0) rectangle (13, 1) node (ge) [pos=.5] {... $> p$ ...};
      \draw (0, 2) node (pivot) {pivot $p$}
            (5, 2) node (left) {$\geq p$}
            (8, 2) node (right) {$\leq p$};
      \draw[thick, ->] (pivot) edge (xl)
                       (left) edge [bend right] (xi)
                       (right) edge [bend right] (xj);
      \end{tikzpicture}} \\
   \subcaptionbox{Partition invariant}{
      \begin{tikzpicture}[scale=0.8]
      \draw (0, 0) rectangle (1, 1) node (xl) [pos=.5] {x[l]}
            (1, 0) rectangle (5, 1) node (leq) [pos=.5] {... $\leq p$ ...}
            (5, 0) rectangle (7, 1) node (rest) [pos=.5] {... ? ...}
            (7, 0) rectangle (11, 1) node (ge) [pos=.5] {... $\geq p$ ...};
      \fill [black] (5, 0) rectangle (5.1, 1) node (ibar) [pos=.5] {}
                    (7, 0) rectangle (7.1, 1) node (jbar) [pos=.5] {};
      \draw (0, 2) node (pivot) {pivot $p$}
            (5, 2) node (i) {$i$}
            (7, 2) node (j) {$j$};
      \draw[thick, ->] (pivot) edge (xl)
                       (i) edge [bend right] (ibar)
                       (j) edge [bend right] (jbar);
      \end{tikzpicture}} \\
   \caption{2-way scan}
   \label{fig:partition-2-way}
\end{figure}

When $i$ meets $j$, we need an extra exchange, swap the pivot $p$ to position $j$. Then recursive sort sub-array $A[l ... j)$ and $A[i ... u)$.

\begin{algorithmic}[1]
\Procedure{Sort}{$A, l, u$} \Comment{sort range $[l, u)$}
  \If{$u - l > 1$} \Comment{At least 2 elements}
    \State $i \gets l$, $j \gets u$
    \State $pivot \gets A[l]$
    \Loop
      \Repeat
        \State $i \gets i + 1$
      \Until{$A[i] \geq pivot$} \Comment{Ignore $i \geq u$}
      \Repeat
        \State $j \gets j - 1$
      \Until{$A[j] \leq pivot$} \Comment{Ignore $j < l$}
      \If{$j < i$}
        \State break
      \EndIf
      \State \textproc{Exchange} $A[i] \leftrightarrow A[j]$
    \EndLoop
    \State \textproc{Exchange} $A[l] \leftrightarrow A[j]$ \Comment{Move the pivot}
    \State \Call{Sort}{$A, l, j$}
    \State \Call{Sort}{$A, i, u$}
  \EndIf
\EndProcedure
\end{algorithmic}

\index{Quick Sort!3-way partition}
Consider the special case that all elements are equal, the array is partitioned into two same parts with $\dfrac{n}{2}$ swaps. Because of the balanced partition, the performance is $O(n \lg n)$. It takes less swaps than the one pass scan method, since it skips the elements on the right side of the pivot. We can combine 2-way scan and ternary partition. Only recursively sort the elements different with the pivot. Jon Bentley and Douglas McIlroy developed a method as shown in figure \ref{fig:partition-3-way} (a), that store the elements equal to the pivot on both sides\cite{3-way-part}\cite{opt-qs}.

\begin{figure}[htbp]
   \centering
   \subcaptionbox{Ternary partition invariant.}{
      \begin{tikzpicture}[scale=0.8]
      \draw (0, 0) rectangle (1, 1) node (xl) [pos=.5] {$x[l]$}
            (1, 0) rectangle (3, 1) node [pos=.5] {... $=$ ...}
            (3, 0) rectangle (5, 1) node [pos=.5] {... $<$ ...}
            (5, 0) rectangle (7, 1) node [pos=.5] {... ? ...}
            (7, 0) rectangle (9, 1) node [pos=.5] {... $>$ ...}
            (9, 0) rectangle (11, 1) node [pos=.5] {... $=$ ...};
      \fill [black] (3, 0) rectangle (3.1, 1) node (pbar) [pos=.5] {}
                    (5, 0) rectangle (5.1, 1) node (ibar) [pos=.5] {}
                    (7, 0) rectangle (7.1, 1) node (jbar) [pos=.5] {}
                    (9, 0) rectangle (9.1, 1) node (qbar) [pos=.5] {};
      \draw (0, 2) node (pivot) {pivot}
            (3, 2) node (p) {$p$}
            (5, 2) node (i) {$i$}
            (7, 2) node (j) {$j$}
            (9, 2) node (q) {$q$};
      \draw[thick, ->] (pivot) edge (xl)
                       (p) edge [bend right] (pbar)
                       (i) edge [bend right] (ibar)
                       (j) edge [bend left] (jbar)
                       (q) edge [bend left] (qbar);
      \end{tikzpicture}} \\
   \subcaptionbox{Swap the elements $= p$ to the middle.}{\hspace{0.1\textwidth}
      \begin{tikzpicture}[scale=0.8]
      \draw (0, 0) rectangle (2, 1) node [pos=.5] {... $<$ ...}
            (2, 0) rectangle (4, 1) node [pos=.5] {... $=$ ...}
            (4, 0) rectangle (6, 1) node [pos=.5] {... $>$ ...};
      \fill [black] (2, 0) rectangle (2.1, 1) node (ibar) [pos=.5] {}
                    (4, 0) rectangle (4.1, 1) node (jbar) [pos=.5] {};
      \draw (2, 2) node (i) {$i$}
            (4, 2) node (j) {$j$};
      \draw[thick, ->] (i) edge [bend right] (ibar)
                       (j) edge [bend left] (jbar);
      \end{tikzpicture}
      \hspace{0.1\textwidth}}
   \caption{Ternary partition}
   \label{fig:partition-3-way}
\end{figure}

We scan from two sides, pause when $i$ reach an element $\geq$ the pivot, and $j$ reach one $\leq$ the pivot. If $i$ doesn't meet or pass $j$, we exchange $A[i] \leftrightarrow A[j]$, then check if $A[i]$ or $A[j]$ equals to the pivot. If yes, we exchange $A[i] \leftrightarrow A[p]$ or $A[j] \leftrightarrow A[q]$ respectively. Finally, we swap all the elements equal to the pivot to the middle. This step do nothing if all elements are unique. The partition result is shown as \ref{fig:partition-3-way} (b). We next only recursively sort the elements not equal to the pivot.

\begin{algorithmic}[1]
\Procedure{Sort}{$A, l, u$}
  \If{$u - l > 1$}
    \State $i \gets l$, $j \gets u$
    \State $p \gets l$, $q \gets u$ \Comment{point to the boundaries of duplicated elements}
    \State $pivot \gets A[l]$
    \Loop
      \Repeat
        \State $i \gets i + 1$
      \Until{$A[i] \geq pivot$} \Comment{Ignore $i \geq u$ case}
      \Repeat
        \State $j \gets j - 1$
      \Until{$A[j] \leq pivot$} \Comment{Ignore $j < l$ case}
      \If{$j \leq i$}
        \State break
      \EndIf
      \State \textproc{Exchange} $A[i] \leftrightarrow A[j]$
      \If{$A[i] = pivot$} \Comment{duplicated element}
        \State $p \gets p + 1$
        \State \textproc{Exchange} $A[p] \leftrightarrow A[i]$
      \EndIf
      \If{$A[j] = pivot$}
        \State $q \gets q - 1$
        \State \textproc{Exchange} $A[q] \leftrightarrow A[j]$
      \EndIf
    \EndLoop
    \If{$i = j$ and $A[i] = pivot$}
      \State $j \gets j - 1$, $i \gets i + 1$
    \EndIf
    \For{$k$ from $l$ to $p$} \Comment{Swap the duplicated elements to the middle}
      \State \textproc{Exchange} $A[k] \leftrightarrow A[j]$
      \State $j \gets j - 1$
    \EndFor
    \For{$k$ from $u-1$ down-to $q$}
      \State \textproc{Exchange} $A[k] \leftrightarrow A[i]$
      \State $i \gets i + 1$
    \EndFor
    \State \Call{Sort}{$A, l, j + 1$}
    \State \Call{Sort}{$A, i, u$}
  \EndIf
\EndProcedure
\end{algorithmic}

It becomes complex when combine 2-way scan and ternary partition. We can change the one pass scan to ternary partition directly. Pick the first element as the pivot, as shown in figure \ref{fig:partition-3-way-lumoto}. At any time, the left part contains elements $< p$; the next part contains those $= p$; and the right part contains those $> p$. The boundaries are $i, k, j$. Elements between $[k, j)$ are yet to be partitioned. We scan from left to right. When start, the part $< p$ is empty; the part $= p$ has an element; $i$ points to the lower boundary, $k$ points to the next. The part $> p$ is empty too, $j$ points to the upper boundary.

\begin{figure}[htbp]
   \centering
      \begin{tikzpicture}[scale=0.8]
      \draw (0, 0) rectangle (2, 1) node [pos=.5] {... $< p$ ...}
            (2, 0) rectangle (4, 1) node [pos=.5] {... $= p$ ...}
            (4, 0) rectangle (6, 1) node [pos=.5] {... ? ...}
            (6, 0) rectangle (8, 1) node [pos=.5] {... $> p$ ...};
      \fill [black] (2, 0) rectangle (2.1, 1) node (ibar) [pos=.5] {}
                    (4, 0) rectangle (4.1, 1) node (kbar) [pos=.5] {}
                    (6, 0) rectangle (6.1, 1) node (jbar) [pos=.5] {};
      \draw (2, 2) node (i) {$i$}
            (4, 2) node (k) {$k$}
            (6, 2) node (j) {$j$};
      \draw[thick, ->] (i) edge [bend right] (ibar)
                       (k) edge [bend right] (kbar)
                       (j) edge [bend left] (jbar);
      \end{tikzpicture}
   \caption{1 way scan ternary partition}
   \label{fig:partition-3-way-lomuto}
\end{figure}

We iterate on $k$, if $A[k] = p$, then move $k$ to the next; if $A[k] > p$, then exchange $A[k] \leftrightarrow A[j-1]$, the range of elements that $> p$ increases by one. Its boundary $j$ moves to left a step. Because we don't know if the element moved to $k$ is still $> p$, we compare again and repeat. Otherwise if $A[k] < p$, we exchange $A[k] \leftrightarrow A[i]$, where $A[i]$ is the first element that $= p$. The partition terminates when $k$ meets $j$.

\begin{algorithmic}[1]
\Procedure{Sort}{$A, l, u$}
  \If{$u - l > 1$}
    \State $i \gets l$, $j \gets u$, $k \gets l + 1$
    \State $pivot \gets A[i]$
    \While{$k < j$}
      \While{$pivot < A[k]$}
        \State $j \gets j - 1$
        \State \textproc{Exchange} $A[k] \leftrightarrow A[j]$
      \EndWhile
      \If{$A[k] < pivot$}
        \State \textproc{Exchange} $A[k] \leftrightarrow A[i]$
        \State $i \gets i + 1$
      \EndIf
      \State $k \gets k + 1$
    \EndWhile
    \State \Call{Sort}{$A, l, i$}
    \State \Call{Sort}{$A, j, u$}
  \EndIf
\EndProcedure
\end{algorithmic}

Compare with the ternary partition through 2-way scan, this implementation is less complex but need more swaps.

\subsubsection{Worst cases}

Although ternary partition handles duplicated elements well, there are the worst cases. For example, when most elements are ordered (ascending or descending), the partition is unbalanced. Figure \ref{fig:worst-cases-1} gives two of the worst cases: $[x_1 < x_2 < ... < x_n]$ and $[y_1 > y_2 > ... > y_n]$. It's easy to give more, for example: $[x_m, x_{m-1}, ..., x_2, x_1, x_{m+1}, x_{m+2}, ... x_n]$, where $[ x_1 < x_2 < ... < x_n]$, and $[x_n, x_1, x_{n-1}, x_2, ... ]$ as shown in figure \ref{fig:worst-cases-2}.

\begin{figure}[htbp]
   \centering
   \subcaptionbox{Partition tree of $[x_1 < x_2 < ... < x_n]$, the sub-trees of $\leq p$ are empty.}{\hspace{.3\textwidth} \includegraphics[scale=0.5]{img/unbalanced} \hspace{.3\textwidth}} \\
   \subcaptionbox{Partition tree of $[y_1 > y_2 > ... > y_n]$, the sub-trees of $\geq p$ are empty.}{\includegraphics[scale=0.5]{img/unbalanced-2}} \\
   \caption{The worst cases - 1.}
   \label{fig:worst-cases-1}
\end{figure}

\begin{figure}[htbp]
   \centering
   \subcaptionbox{Unbalanced partitions except for the first time.}{\includegraphics[scale=0.4]{img/unbalanced-3}} \\
   \subcaptionbox{A zig-zag partition tree.}{\includegraphics[scale=0.5]{img/unbalanced-zigzag}} \\
   \caption{The worst cases - 2.}
   \label{fig:worst-cases-2}
\end{figure}

In these worst cases, the partition is unbalanced when choose the first element as the pivot. Robert Sedgwick improved the pivot selection\cite{qsort-impl}: Instead pick a fixed position, sample several elements to avoid bad pivot. We sample the first, the middle, and the last, pick the median as the pivot. We can either compare every two (total 3 times)\cite{3-way-part}, or swap the least one to head, swap the greatest one end, and move the median to the middle.

\begin{algorithmic}[1]
\Procedure{Sort}{$A, l, u$}
  \If{$u - l > 1$}
    \State $m \gets \lfloor \dfrac{l + u}{2} \rfloor$ \Comment{or $l + \dfrac{u - l}{2}$ to void overflow}
    \If{$A[m] < A[l]$} \Comment{Ensure $A[l] \leq A[m]$}
      \State \textproc{Exchange} $A[l] \leftrightarrow A[m]$
    \EndIf
    \If{$A[u-1] < A[l]$} \Comment{Ensure $A[l] \leq A[u-1]$}
      \State \textproc{Exchange} $A[l] \leftrightarrow A[u-1]$
    \EndIf
    \If{$A[u-1] < A[m]$} \Comment{Ensure $A[m] \leq A[u-1]$}
      \State \textproc{Exchange} $A[m] \leftrightarrow A[u-1]$
    \EndIf
    \State \textproc{Exchange} $A[l] \leftrightarrow A[m]$
    \State $(i, j) \gets $ \Call{Partition}{$A, l, u$}
    \State \Call{Sort}{$A, l, i$}
    \State \Call{Sort}{$A, j, u$}
  \EndIf
\EndProcedure
\end{algorithmic}

This implementation handles the above four worst cases well. We call it `median of three'. Alternatively, we can randomly pick pivot:

\begin{algorithmic}[1]
\Procedure{Sort}{$A, l, u$}
  \If{$u - l > 1$}
    \State \textproc{Exchange} $A[l] \leftrightarrow A[$ \Call{Random}{$l, u$} $]$
    \State $(i, j) \gets $ \Call{Partition}{$A, l, u$}
    \State \Call{Sort}{$A, l, i$}
    \State \Call{Sort}{$A, j, u$}
  \EndIf
\EndProcedure
\end{algorithmic}

Where \textproc{Random}($l, u$) returns integer $l \leq i < u$ randomly. We swap $A[i]$ with the first element as the pivot. This method is called {\em random quick sort} \cite{CLRS}. Theoretically, neither `median of three' nor random quick sort can avoid the worst case completely. If the sequence is random, it's same to choose any one as the pivot. Nonetheless, these improvements are widely used in engineering practice.

There are other improvements besides partition. Sedgewick found quick sort had overhead when the list is short, while insert sort performed better\cite{Bentley}\cite{3-way-part}. Sedgewick, Bentley and McIlroy evaluated varies thresholds, as `cut-off'. When the elements are less than the `cut-off', then switch to insert sort.

\begin{algorithmic}[1]
\Procedure{Sort}{$A, l, u$}
  \If{$u - l > $ \textproc{Cut-Off}}
    \State \Call{Quick-Sort}{$A, l, u$}
  \Else
    \State \Call{Insertion-Sort}{$A, l, u$}
  \EndIf
\EndProcedure
\end{algorithmic}

\subsection{quick sort and tree sort}

The `true quick sort' is the combination of multiple engineering improvements, falls back to insert sort for small sequence, in-place swaps, choose the pivot as the `median of three', 2-way scan, and ternary partition. Some people think the basic recursive definition is essentially tree sort. Richard Bird derived quick sort from binary tree sort by deforestation\cite{algo-fp}. Define \textit{unfold} that converts a list to binary search tree:

\be
\begin{array}{rcl}
\textit{unfold}\ [\ ] & = & \nil \\
\textit{unfold}\ (x \cons xs) & = & (\textit{unfold}\ [a | a \in xs, a \leq x], x, \textit{unfold}\ [a | a \in xs, a > x]) \\
\end{array}
\ee

Compare with the binary tree insert (see chapter 2), \textit{unfold} creates the tree differently. If the list is empty, the tree is empty; otherwise, use the first element $x$ as the key, then recursively build the left, right sub-trees. Where the left sub-tree has the elements $\leq x$; and the right tree has elements that $> x$. While to convert a binary search tree to ordered list, we define in-order traverse as:

\be
\begin{array}{rcl}
\textit{toList}\ \nil & = & [\ ] \\
\textit{toList}\ (l, k, r) & = & \textit{toList}\ l \doubleplus [k] \doubleplus \textit{toList}\ r \\
\end{array}
\ee

We define quick sort by composing the two functions:

\be
\textit{sort} = \textit{toList} \circ \textit{unfold}
\ee

We first build the binary search tree through \textit{unfold}, then pass it to \textit{toList} to generate the list, and discard the tree. When eliminate the intermediate tree (through {\em deforestation} by Burstle-Darlington's work\cite{slpj}), we obtain the quick sort.

\section{Merge sort}
\index{Merge Sort}

Quick sort performs well in most cases. However, there are the worst cases can't be completely avoided. Merge sort guarantees $O(n \lg n)$ performance in all cases. It supports both arrays and lists. Many programming environments provide merge sort as the standard sort tool\footnote{For example in the standard library of Haskell, Python, and Java.}. Merge sort takes divide and conquer approach. It always splits the sequence in half and half, recursively sort them and merge.

\be
\begin{array}{rcl}
sort\ [\ ] & = & [\ ] \\
sort\ [x] & = & [x] \\
sort\ xs & = & merge\ (sort\ as)\ (sort\ bs), \text{where}: (as, bs) = \textit{halve}\ xs
\end{array}
\ee

Where \textit{halve} splits the sequence, for array, we can cut at the middle: $\textit{splitAt}\ \lfloor \dfrac{|xs|}{2} \rfloor\ xs$. However, it takes linear time to move to the middle point of a list (see chapter 1):

\be
\textit{splitAt}\ n\ xs = \textit{shift}\ n\ [\ ]\ xs
\ee

Where:

\be
\begin{array}{rcl}
\textit{shift}\ 0\ as\ bs & = & (as, bs) \\
\textit{shift}\ n\ as\ (b \cons bs) & = & \textit{shift}\ (n - 1)\ (b \cons as)\ bs
\end{array}
\ee

Because \textit{halve} needn't keep the relative order among elements, we can simplify the implementation with odd-even split. There are same number of elements in odd and even positions, or they only differ by one. $\textit{halve} = \textit{split}\ [\ ]\ [\ ]$, where:

\be
\begin{array}{rcl}
\textit{split}\ as\ bs\ [\ ] & = & (as, bs) \\
\textit{split}\ as\ bs\ [x] & = & (x \cons as, bs) \\
\textit{split}\ as\ bs\ (x \cons y \cons xs) & = & \textit{split}\ (x \cons as)\ (y \cons bs)\ xs \\
\end{array}
\ee

We can further simplify it with folding, as in below example, we add $x$ to $a$ every time, then swap $as \leftrightarrow bs$:

\begin{Haskell}
halve = foldr f ([], []) where
  f x (as, bs) = (bs, x : as)
\end{Haskell}

\subsection{Merge}
\index{Merge Sort!Merge}

Merge is demonstrated as figure \ref{fig:merge}. Consider two groups of kids, already ordered from short to tall. They need pass a gate, one kid per time. We arrange the first kid from each group to compare, the shorter one pass the gate. Repeat this till a group pass the gate, then the remaining kids pass the gate one by one.

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.3]{img/merge2w}
 \caption{Merge}
 \label{fig:merge}
\end{figure}

\be
\begin{array}{rcl}
\textit{merge}\ [\ ]\ bs & = & bs \\
\textit{merge}\ as\ [\ ] & = & as \\
\textit{merge}\ (a \cons as)\ (b \cons bs) & = & \begin{cases}
  a < b: & a : \textit{merge}\ as\ (b \cons bs) \\
  \text{otherwise}: & b : \textit{merge}\ (a \cons as)\ bs
  \end{cases}
\end{array}
\ee

For array, we directly cut at the middle position, recursively sort two halves, then merge:

\begin{algorithmic}[1]
\Procedure{Sort}{$A$}
  \State $n \gets |A|$
  \If{$n > 1$}
    \State $m \gets \lfloor \dfrac{n}{2} \rfloor$
    \State $X \gets$ \Call{Copy-Array}{$A[1...m]$}
    \State $Y \gets$ \Call{Copy-Array}{$A[m+1...n]$}
    \State \Call{Sort}{$X$}
    \State \Call{Sort}{$Y$}
    \State \Call{Merge}{$A, X, Y$}
  \EndIf
\EndProcedure
\end{algorithmic}

We allocated additional space of the same size of $A$ because \textproc{Merge} is not in-pace. We repeatedly compare elements from $X$ and $Y$, pick the less one to $A$. When either sub-array finish, we add all the remaining to $A$.

\begin{algorithmic}[1]
\Procedure{Merge}{$A, X, Y$}
  \State $i \gets 1, j\gets 1, k\gets 1$
  \State $m \gets |X|, n \gets |Y|$
  \While{$i \leq m$ and $j \leq n$}
    \If{$X[i] < Y[j]$}
      \State $A[k] \gets X[i]$
      \State $i \gets i + 1$
    \Else
      \State $A[k] \gets Y[j]$
      \State $j \gets j + 1$
    \EndIf
    \State $k \gets k + 1$
  \EndWhile
  \While{$i \leq m$}
    \State $A[k] \gets X[i]$
    \State $k \gets k + 1$
    \State $i \gets i + 1$
  \EndWhile
  \While{$j \leq n$}
    \State $A[k] \gets Y[j]$
    \State $k \gets k + 1$
    \State $j \gets j + 1$
  \EndWhile
\EndProcedure
\end{algorithmic}

\subsection{Performance}
\index{Merge Sort!Performance}

Merge sort has two steps: partition and merge. We always halve the sequence. The partition tree is a balanced binary tree as shown in figure \ref{fig:qsort-best}. The height is $O(\lg n)$, so as the recursion depth. The merge happens at every level, compares elements one by one from each sorted sub-sequence. Hence merge takes linear time. For sequence of length $n$, let $T(n)$ be the merge sort time, we have below recursive breakdown:

\be
T(n) = T(\dfrac{n}{2}) + T(\dfrac{n}{2}) + c n = 2 T(\dfrac{n}{2}) + c n
\ee

The time consists of three parts: sort the first half, sort the second half, each takes $T(\dfrac{n}{2})$ time; and merge in $c n$ time, where $c$ is a constant. Solving this equation gives $O(n \lg n)$ result. The other performance factor is space. Varies implementation differ a lot. The basic merge sort allocates the space of the same size as the array in each recursion, copies elements and sorts, then release the space. When reach to the deepest recursion, consume the largest space of $O(n \lg n)$.

\subsubsection{Improvement}
\index{Merge Sort!Work area}

To simplify merge, we append $\infty$ to $X$ and $Y$\footnote{$-\infty$ for descending order}.

\begin{algorithmic}[1]
\Procedure{Merge}{$A, X, Y$}
  \State \Call{Append}{$X, \infty$}
  \State \Call{Append}{$Y, \infty$}
  \State $i \gets 1, j\gets 1, n \gets |A|$
  \For{$k \gets$ from 1 to $n$}
    \If{$X[i] < Y[j]$}
      \State $A[k] \gets X[i]$
      \State $i \gets i + 1$
    \Else
      \State $A[k] \gets Y[j]$
      \State $j \gets j + 1$
    \EndIf
  \EndFor
\EndProcedure
\end{algorithmic}

It's expensive to allocate/release space repeatedly\cite{Bentley}. We can pre-allocate a work area of the same size as $A$. Reuse it during recursive merge, and finally release it.

\begin{algorithmic}[1]
\Procedure{Sort}{A}
  \State $n \gets |A|$
  \State \textproc{Sort$'$}$(A$, \Call{Create-Array}{$n$}, $1, n)$
\EndProcedure
\Statex
\Procedure{Sort$'$}{$A, B, l, u$}
  \If{$u - l > 0$}
    \State $m \gets \lfloor \dfrac{l + u}{2} \rfloor$
    \State \Call{Sort$'$}{$A, B, l, m$}
    \State \Call{Sort$'$}{$A, B, m + 1, u$}
    \State \Call{Merge$'$}{$A, B, l, m, u$}
  \EndIf
\EndProcedure
\end{algorithmic}

We need update \textproc{Merge$'$} with the passed in work area:

\begin{algorithmic}[1]
\Procedure{Merge$'$}{$A, B, l, m, u$}
  \State $i \gets l, j \gets m + 1, k \gets l$
  \While{$i \leq m$ and $j \leq u$}
    \If{$A[i] < A[j]$}
      \State $B[k] \gets A[i]$
      \State $i \gets i + 1$
    \Else
      \State $B[k] \gets A[j]$
      \State $j \gets j + 1$
    \EndIf
    \State $k \gets k + 1$
  \EndWhile
  \While{$i \leq m$}
    \State $B[k] \gets A[i]$
    \State $k \gets k + 1$
    \State $i \gets i + 1$
  \EndWhile
  \While{$j \leq u$}
    \State $B[k] \gets A[j]$
    \State $k \gets k + 1$
    \State $j \gets j + 1$
  \EndWhile
  \For{$i \gets$ from $l$ to $u$} \Comment{copy back}
    \State $A[i] \gets B[i]$
  \EndFor
\EndProcedure
\end{algorithmic}

This implementation reduces the space from $O(n \lg n)$ to $O(n)$, improve performance 20\% to 25\% for 100,000 numeric elements.

\subsection{In-place merge sort}
\index{Merge Sort!In-place merge sort}

To avoid additional space, we consider how to reuse the array as the work area. As shown in figure \ref{fig:merge-in-place-naive}, sub-array $A$ and $B$ are sorted, when merge in-place, the part before $l$ are merged and ordered. If $A[l] < A[m]$, move $l$ to right a step; otherwise if $A[l] \geq A[m]$, we need move $A[m]$ to merge result before $l$. We need shift all elements between $l$ and $m$ (including $l$) to right a step.

\begin{figure}[htbp]
 \centering
      \begin{tikzpicture}[scale=0.8]
      \draw (0, 0) rectangle (3, 1) node [pos=.5] {merged}
            (3, 0) rectangle (4, 1) node [pos=.5] {$A[l]$}
            (4, 0) rectangle (8, 1) node (subA) [pos=.5] {... sorted $X$...}
            (8, 0) rectangle (9, 1) node (xsj) [pos=.5] {$A[m]$}
            (9, 0) rectangle (13, 1) node [pos=.5] {... sorted $Y$...};
      \draw[thick, ->] (subA) edge [bend right=45] node [below] {if $A[l] \geq A[m]$, then shift $X$} (xsj);
      \end{tikzpicture}
 \caption{In-place shift and merge}
 \label{fig:merge-in-place-naive}
\end{figure}

\begin{algorithmic}[1]
\Procedure{Merge}{$A, l, m, u$}
  \While{$l \leq m \land m \leq u$}
    \If{$A[l] < A[m]$}
      \State $l \gets l + 1$
    \Else
      \State $x \gets A[m]$
      \For{$i \gets m $ down-to $l+1$} \Comment{Shift}
        \State $A[i] \gets A[i-1]$
      \EndFor
      \State $A[l] \gets x$
    \EndIf
  \EndWhile
\EndProcedure
\end{algorithmic}

\index{Merge Sort!Work area}
However, the in-place shift and merge downgrades the performance to $O(n^2)$ time. Array shift takes linear time, proportion to the length of $X$. When sort a sub-array, our idea is to reuse the remaining part as the work area, and avoid overwriting the elements in it. When compare elements from sorted sub-array $A$ and $B$, we chose the less one and store it in the work area, but we need exchange the element out to free up the cell. After merge, $A$ and $B$ together store the content of the original work area, as shown in figure \ref{fig:merge-workarea}.

\begin{figure}[htbp]
 \centering
      \begin{tikzpicture}[scale=0.8]
      \draw (0, 2) rectangle (3, 3) node [pos=.5] {... reuse ...}
            (3, 2) rectangle (4, 3) node (ai) [pos=.5] {$A[i]$}
            (4, 2) rectangle (5, 3) node [pos=.5] {...}
            (7, 2) rectangle (10, 3) node [pos=.5] {... reuse ...}
            (10, 2) rectangle (11, 3) node (bj) [pos=.5] {$B[j]$}
            (11, 2) rectangle (12, 3) node [pos=.5] {...}
            (0, 0) rectangle (3, 1) node [pos=.5] {... merged ...}
            (3, 0) rectangle (4, 1) node (ck) [pos=.5] {$C[k]$}
            (4, 0) rectangle (5, 1) node [pos=.5] {...};
      \draw[thick, <->] (ai) edge [bend left] node [above] {compare} (bj)
                        (ai) edge node [right] {if $A[i] < B[j]$ then exchange $A[i] \leftrightarrow C[k]$} (ck);
      \end{tikzpicture}
 \caption{Merge and swap}
 \label{fig:merge-workarea}
\end{figure}

The sorted array $A$, $B$, and work area $C$ are all part of the array. We pass the start, end positions of $A$ and $B$ as ranges $[i, m)$, $[j, n)$\footnote{range $[a, b)$ includes $a$, but excludes $b$.}. The work area starts from $k$.

\begin{algorithmic}[1]
\Procedure{Merge}{$A, [i, m), [j, n), k$}
  \While{$i < m$ and $j < n$}
    \If{$A[i] < A[j]$}
      \State \textproc{Exchange} $A[k] \leftrightarrow A[i]$
      \State $i \gets i + 1$
    \Else
      \State \textproc{Exchange} $A[k] \leftrightarrow A[j]$
      \State $j \gets j + 1$
    \EndIf
    \State $k \gets k + 1$
  \EndWhile
  \While{$i < m$}
    \State \textproc{Exchange} $A[k] \leftrightarrow A[i]$
    \State $i \gets i + 1$
    \State $k \gets k + 1$
  \EndWhile
  \While{$j < m$}
    \State \textproc{Exchange} $A[k] \leftrightarrow A[j]$
    \State $j \gets j + 1$
    \State $k \gets k + 1$
  \EndWhile
\EndProcedure
\end{algorithmic}

The work area satisfies below two rules:

\begin{enumerate}
\item The work area has sufficient size to hold elements swapped in;
\item The work area can overlap with either sorted sub-arrays, but not overwrite any unmerged elements.
\end{enumerate}

We can use half array as the work area to sort the other half, as shown in figure \ref{fig:merge-in-place-start}.

\begin{figure}[htbp]
 \centering
      \begin{tikzpicture}[scale=0.8]
      \draw (0, 0) rectangle (3, 1) node [pos=.5] {... unsorted ...}
            (3, 0) rectangle (6, 1) node [pos=.5] {... sorted ...};
      \end{tikzpicture} \\
 \caption{Merge and sort half array}
 \label{fig:merge-in-place-start}
\end{figure}

We next sort further half of the work area (remaining $\dfrac{1}{4}$), as shown in figure \ref{fig:merge-in-place-quater}. We must merge $A$ ($\dfrac{1}{2}$ array) and $B$ ($\dfrac{1}{4}$ array) later sometime. However, the work area can only hold $\dfrac{1}{4}$ array, insufficient for size of $A + B$.

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.8]{img/workarea-1}
 \caption{Work area can't support merge $A$ and $B$.}
 \label{fig:merge-in-place-quater}
\end{figure}

The second rule gives us an opportunity: arrange the work area overlapped with either sub-array, and only override the merged part. We first sort the second 1/2 of the work area, as the result, swap $B$ to the first 1/2, the new work area is between $A$ and $B$, as shown in the upper of figure \ref{fig:merge-in-place-setup}. The work area is overlapped with $A$\cite{msort-in-place}. Consider two extremes:

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.8]{img/workarea-2}
 \caption{Merge $A$ and $B$ with the work area.}
 \label{fig:merge-in-place-setup}
\end{figure}

\begin{enumerate}
\item $x < y$, for all $x$ in $B$, $y$ in $A$. After merge, contents of $B$ and the work area are swapped (the size of $B$ equals to the work area);
\item $y < x$, for all $x$ in $B$, $y$ in $A$. During merge, we repeatedly swap content of $A$ and the work area. After half of $A$ is swapped, we start overriding $A$. Fortunately, we only override the merged content. The right boundary of work area keep moving to the 3/4 of the array. After that, we start swap the content of $B$ and the work area. Finally, the work area moves to the left side of the array, as shown in the bottom of figure \ref{fig:merge-in-place-setup} (b).
\end{enumerate}

The other cases are between the above two extremes. The work area finally moves to the first 1/4 of the array. Repeat this, we always sort the second 1/2 of the work area, swap the result to the first 1/2, and keep the work area in the middle. We halve the work area every time $\dfrac{1}{2}, \dfrac{1}{4}, \dfrac{1}{8}, ...$ of the array, terminate when there is only one element left. We cal also switch to insert sort for the last few elements.

\begin{algorithmic}[1]
\Procedure{Sort}{$A, l, u$}
  \If{$u - l > 0$}
    \State $m \gets \lfloor \dfrac{l + u}{2} \rfloor$
    \State $w \gets l + u - m$
    \State \Call{Sort'}{$A, l, m, w$} \Comment{sort half}
    \While{$w - l > 1$}
      \State $u' \gets w$
      \State $w \gets \lceil \dfrac{l + u'}{2} \rceil$ \Comment{halve the work area}
      \State \Call{Sort'}{$A, w, u', l$} \Comment{sort the remaining half}
      \State \Call{Merge}{$A, [l, l + u' - w], [u', u], w$}
    \EndWhile
    \For{$i \gets w$ down-to $l$} \Comment{Switch to insert sort}
      \State $j \gets i$
      \While{$j \leq u$ and $A[j] < A[j-1]$}
        \State \textproc{Exchange} $A[j] \leftrightarrow A[j-1]$
        \State $j \gets j + 1$
      \EndWhile
    \EndFor
  \EndIf
\EndProcedure
\end{algorithmic}

We round up the work area to ensure sufficient size, then pass the range and work area to \textproc{Merge}. We next update \textproc{Sort'}, which calls \textproc{Sort} to swap the work area and merged part.

\begin{algorithmic}[1]
\Procedure{Sort'}{$A, l, u, w$}
  \If{$u - l > 0$}
    \State $m \gets \lfloor \dfrac{l + u}{2} \rfloor$
    \State \Call{Sort}{$A, l, m$}
    \State \Call{Sort}{$A, m+1, u$}
    \State \Call{Merge}{$A, [l, m), [m+1, u), w$}
  \Else \Comment{Swap elements to the work area}
    \While{$l \leq u$}
      \State \textproc{Exchange} $A[l] \leftrightarrow A[w]$
      \State $l \gets l + 1$
      \State $w \gets w + 1$
    \EndWhile
  \EndIf
\EndProcedure
\end{algorithmic}

This implementation needn't shift sub-array, it keeps reducing the unordered part: $\dfrac{n}{2}, \dfrac{n}{4}, \dfrac{n}{8}, ...$, completes in $O(\lg n)$ steps. Every step sorts half of the remaining, then merge in linear time. Let the time to sort $n$ elements be $T(n)$, we have the following recursive result:

\be
T(n) = T(\frac{n}{2}) + c \frac{n}{2} + T(\frac{n}{4}) + c \frac{3n}{4} + T(\frac{n}{8}) + c \frac{7n}{8} + ...
\label{eq:in-place-sort-time}
\ee

For half elements, the time is:

\be
T(\frac{n}{2}) = T(\frac{n}{4}) + c \frac{n}{4} + T(\frac{n}{8}) + c \frac{3n}{8} + T(\frac{n}{16}) + c \frac{7n}{16} + ...
\label{eq:in-place-sort-time-half}
\ee

Subtract (\ref{eq:in-place-sort-time}) and (\ref{eq:in-place-sort-time-half}):

\[
T(n) - T(\frac{n}{2}) = T(\frac{n}{2}) + c n (\frac{1}{2} + \frac{1}{2} + ... )
\]

Add $\dfrac{1}{2}$ total $\lg n$ times, hence:

\[
T(n) = 2 T(\frac{1}{2}) + \frac{c}{2} n \lg n
\]

Apply telescope method, obtain the result $O(n \lg^2 n)$.

\section{Nature merge sort}
\index{Merge Sort!Nature merge sort}

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.25]{img/burn-candle-2-ends}
 \caption{Burn from both ends}
 \label{fig:burn-candle}
\end{figure}

Knuth gives another implementation, called {\em nature merge sort}. It likes burning a candle from both ends\cite{TAOCP}. For any sequence, one can always find a ordered segment from any position. Particularly, we can find such a segment from left end as shown in below table.

\btab{l}
\underline{15}, 0, 4, 3, 5, 2, 7, 1, 12, 14, 13, 8, 9, 6, 10, 11 \\
\underline{8, 12, 14}, 0, 1, 4, 11, 2, 3, 5, 9, 13, 10, 6, 15, 7 \\
\underline{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15 } \\
\etab

The first row is the extreme case of a singleton segment, the second is less than the first; the third row is the other extreme that the segment extends to the right end, the whole sequence is ordered. Symmetrically, we can always find the ordered segment from right end. We can merge the two sorted segments, one from left, another from right. The advantage is to re-use the nature ordered sub-sequences for partition.

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.8]{img/nature-merge-sort}
 \caption{Nature merge sort}
 \label{fig:nature-merge-sort}
\end{figure}

As shown in figure \ref{fig:nature-merge-sort}, we scan from both ends, find the two longest ordered segments respectively. Then merge them to the left of the work area. Next, we repeat to scan from left and right to center. This time, we merge the two segments for the right to left of the work area. We switch the merge direction right/left in-turns. After scan all elements and merge them to the work area, we swap the original array and the work area, then start a new round of bi-directional scan and merge, terminates when the ordered segment extends to cover the whole array. This implementation process the array from both directions based on nature ordering. We called it {\em nature two-way merge sort}. As shown in figure \ref{fig:nature-msort-invariant}, elements before $a$ and after $d$ are scanned. We span the ordered segment $[a, b)$ to right, meanwhile, span $[c, d)$ to left. For the work area, elements before $f$ and after $f$ are merged (consist of multiple sub-sequences). In odd rounds, we merge $[a, b)$ and $[c, d)$ from $f$ to right; in even rounds, we merge from $r$ to left.

\captionsetup[subfigure]{labelformat=empty, margin=10pt}
\begin{figure}[htbp]
 \centering
   \subcaptionbox{}{
      \begin{tikzpicture}[scale=0.8]
      \draw (0, 0) rectangle (3, 1) node [pos=.5] {... scanned ...}
            (3, 0) rectangle (6, 1) node [pos=.5] {... span $[a, b)$ ...}
            (6, 0) rectangle (8, 1) node [pos=.5] {... ? ...}
            (8, 0) rectangle (11, 1) node [pos=.5] {... span $[c, d)$ ...}
            (11, 0) rectangle (14, 1) node [pos=.5] {... scanned ...};
      \fill [black] (3, 0) rectangle (3.1, 1) node (abar) [pos=.5] {}
                    (6, 0) rectangle (6.1, 1) node (bbar) [pos=.5] {}
                    (8, 0) rectangle (8.1, 1) node (cbar) [pos=.5] {}
                    (11, 0) rectangle (11.1, 1) node (dbar) [pos=.5] {};
      \draw (3, 2) node (a) {a}
            (6, 2) node (b) {b}
            (8, 2) node (c) {c}
            (11, 2) node (d) {d};
      \draw[thick, ->] (a) edge [bend right] (abar)
                       (b) edge [bend right] (bbar)
                       (c) edge [bend left] (cbar)
                       (d) edge [bend left] (dbar);
      \end{tikzpicture}} \\
    \subcaptionbox{}{
      \begin{tikzpicture}[scale=0.8]
      \draw (0, 0) rectangle (3, 1) node [pos=.5] {... merged ...}
            (3, 0) rectangle (8, 1) node [pos=.5] {... free cells ...}
            (8, 0) rectangle (11, 1) node [pos=.5] {... merged ...};
      \fill [black] (3, 0) rectangle (3.1, 1) node (fbar) [pos=.5] {}
                    (8, 0) rectangle (8.1, 1) node (rbar) [pos=.5] {};
      \draw (3, 2) node (f) {f}
            (8, 2) node (r) {r};
      \draw[thick, ->] (f) edge [bend right] (fbar)
                       (r) edge [bend right] (rbar);
      \end{tikzpicture}} \\
 \caption{A status of nature merge sort}
 \label{fig:nature-msort-invariant}
\end{figure}
\captionsetup[subfigure]{labelformat=parens}

When sort starts, we allocate a work area with the same size of the array. $a$ and $b$ point to the left side, $c$ and $d$ point to the right side. $f$ and $r$ point to the two sides of the work area respectively.

\begin{algorithmic}[1]
\Function{Sort}{$A$}
  \If{$|A| > 1$}
    \State $n \gets |A|$
    \State $B \gets$ \Call{Create-Array}{$n$}  \Comment{the work area}
    \Loop
      \State $[a, b) \gets [1, 1)$
      \State $[c, d) \gets [n+1, n+1)$
      \State $f \gets 1, r \gets n$ \Comment{front, rear of the work area}
      \State $t \gets 1$            \Comment{even/odd round}
      \While{$b < c$}               \Comment{elements yet to scan}
        \Repeat \Comment{Span $[a, b)$}
          \State $b \gets b + 1$
        \Until{$b \geq c \lor A[b] < A[b-1]$}

        \Repeat \Comment{Span $[c, d)$}
          \State $c \gets c - 1$
        \Until{$c \leq b \lor A[c-1] < A[c]$}

        \If{$c < b$} \Comment{Avoid overlap}
          \State $c \gets b$
        \EndIf

        \If{$b - a \geq n$} \Comment{Terminates if $[a, b)$ spans the whole array}
          \State \Return $A$
        \EndIf

        \If{$t$ is odd} \Comment{merge to front}
          \State $f \gets$ \Call{Merge}{$A, [a, b), [c, d), B, f, 1$}
        \Else \Comment{merge to rear}
          \State $r \gets$ \Call{Merge}{$A, [a, b), [c, d), B, r, -1$}
        \EndIf
        \State $a \gets b, d \gets c$
        \State $t \gets t + 1$
      \EndWhile
      \State \textproc{Exchange} $A \leftrightarrow B$ \Comment{Switch work area}
    \EndLoop
  \EndIf
  \State \Return $A$
\EndFunction
\end{algorithmic}

We need pass the merge direction in:

\begin{algorithmic}[1]
\Function{Merge}{$A, [a, b), [c, d), B, w, \Delta$}
  \While{$a < b$ and $c < d$}
    \If{$A[a] < A[d-1]$}
      \State $B[w] \gets A[a]$
      \State $a \gets a + 1$
    \Else
      \State $B[w] \gets A[d-1]$
      \State $d \gets d - 1$
    \EndIf
    \State $w \gets w + \Delta$
  \EndWhile
  \While{$a < b$}
    \State $B[w] \gets A[a]$
    \State $a \gets a + 1$
    \State $w \gets w + \Delta$
  \EndWhile
  \While{$c < d$}
    \State $B[w] \gets A[d-1]$
    \State $d \gets d - 1$
    \State $w \gets w + \Delta$
  \EndWhile
  \State \Return $w$
\EndFunction
\end{algorithmic}

The performance does not depend on how ordered the elements are. In the `worst' case, the ordered sub-sequences are all singleton. After merge, the length of the new ordered sub-sequences are at least 2. Suppose we still encounter the `worst' case in the second round, the merged sub-sequences have length at least 4, ... every round double the sub-sequence length, hence we need at most $O(\lg n)$ rounds. Because we can all elements every round, the total time is bound to $O(n \lg n)$. For list, we can't scan from tail back easily as array. A list consists multiple ordered sub-lists, we can merge them in pairs. It halves the sub-lists every round, and finally build the sorted result. We can define this as below (Curried form):

\be
sort = sort' \circ group
\ee

Where $group$ breaks the list into ordered sub-lists:

\be
\begin{array}{rcl}
\textit{group}\ [\ ] & = & [[\ ]] \\
\textit{group}\ [x] & = & [[x]] \\
\textit{group}\ (x \cons y \cons xs) & = & \begin{cases}
  x < y: & (x \cons g) \cons gs, \text{where}: (g \cons gs) = \textit{group}\ (y \cons xs) \\
  \text{otherwise}: & [x] \cons g \cons gs \\
\end{cases}
\end{array}
\ee

\be
\begin{array}{rcl}
\textit{sort}'\ [\ ] & = & [\ ] \\
\textit{sort}'\ [g] & = & g \\
\textit{sort}'\ gs & = & \textit{sort}'\ (\textit{mergePairs}\ gs) \\
\end{array}
\ee

Where \textit{mergePairs} is defined as:

\be
\begin{array}{rcl}
\textit{mergePairs}\ (g_1 \cons g_2 \cons gs) & = & merge\ g_1\ g_2 : \textit{mergePairs}\ gs \\
\textit{mergePairs}\ gs & = & gs
\end{array}
\ee

Alternatively, we can define $sort'$ as fold:

\be
\textit{sort}' = foldr\ merge\ [\ ]
\ee

\begin{Exercise}
\Question{Is the performance of $mergePairs$ and folded merge same? If yes, prove it, if not, which one is faster?}
\end{Exercise}

\section{Bottom-up merge sort}
\index{Merge Sort!Bottom-up merge sort}
The worst case analysis for nature merge sort raises an interesting topic, instead of realizing merge sort in
top-down manner, we can develop a bottom-up version. The great advantage is that, we needn't do book keeping
any more, so the algorithm is quite friendly for purely iterative implementation.

The idea of bottom-up merge sort is to turn the sequence to be sorted into $n$ small sub sequences each contains
only one element. Then we merge every two of such small sub sequences, so that we get $\frac{n}{2}$ ordered
sub sequences each with length 2; If $n$ is odd number, we left the last singleton sequence untouched.
We repeatedly merge these pairs, and finally we get the sorted result. Knuth names this variant as
`straight two-way merge sort' \cite{TAOCP}. The bottom-up merge sort is illustrated in figure \ref{fig:bottom-up-msort}

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.6]{img/bottom-up-msort}
 \caption{Bottom-up merge sort}
 \label{fig:bottom-up-msort}
\end{figure}

Different with the basic version and even-odd version, we needn't explicitly split the list to be sorted
in every recursion. The whole list is split into $n$ singletons at the very beginning, and we merge these
sub lists in the rest of the algorithm.

\be
sort(L) = sort'(wraps(L))
\ee

\be
wraps(L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \phi & L = \phi \\
  \{ \{l_1\} \} \cup wraps(L') & otherwise
  \end{array}
\right.
\ee

Of course $wraps$ can be implemented by using mapping as introduced in appendix A.

\be
sort(L) = sort'(map(\lambda_x \cdot \{ x \}, L))
\ee

We reuse the function $sort'$ and $mergePairs$ which are defined in section of nature merge sort. They
repeatedly merge pairs of sub lists until there is only one.

Implement this version in Haskell gives the following example code.

\lstset{language=Haskell}
\begin{lstlisting}
sort = sort' . map (\x->[x])
\end{lstlisting}

This version is based on what Okasaki presented in \cite{okasaki-book}. It is quite similar to the nature merge sort
only differs in the way of grouping. Actually, it can be deduced as a special case (the worst case) of
nature merge sort by the following equation.

\be
sort(L)= sort'(groupBy(\lambda_{x, y} \cdot False, L))
\ee

That instead of spanning the non-decreasing sub list as long as possible, the predicate always evaluates to false,
so the sub list spans only one element.

Similar with nature merge sort, bottom-up merge sort can also be defined by folding. The detailed implementation
is left as exercise to the reader.

Observing the bottom-up sort, we can find it's in tail-recursion call manner, thus it's quite easy
to translate into purely iterative algorithm without any recursion.

\begin{algorithmic}[1]
\Function{Sort}{$A$}
  \State $B \gets \phi$
  \For{$\forall a \in A$}
    \State $B \gets$ \Call{Append}{$\{ a \}$}
  \EndFor
  \State $N \gets |B|$
  \While{$N > 1$}
    \For{$i \gets $ from $1$ to $\lfloor \frac{N}{2} \rfloor$}
      \State $B[i] \gets$ \Call{Merge}{$B[2i -1], B[2i]$}
    \EndFor
    \If{\Call{Odd}{$N$}}
      \State $B[\lceil \frac{N}{2} \rceil] \gets B[N]$
    \EndIf
    \State $N \gets \lceil \frac{N}{2} \rceil$
  \EndWhile
  \If{$B = \phi$}
    \State \Return $\phi$
  \EndIf
  \State \Return $B[1]$
\EndFunction
\end{algorithmic}

The following example Python program implements the purely iterative bottom-up merge sort.

\lstset{language=Python}
\begin{lstlisting}
def mergesort(xs):
    ys = [[x] for x in xs]
    while len(ys) > 1:
        ys.append(merge(ys.pop(0), ys.pop(0)))
    return [] if ys == [] else ys.pop()

def merge(xs, ys):
    zs = []
    while xs != [] and ys !=[]:
        zs.append(xs.pop(0) if xs[0] < ys[0] else ys.pop(0))
    return zs + (xs if xs !=[] else ys)
\end{lstlisting}

The Python implementation combines multiple rounds of merging by consuming
the pair of lists on the head, and appending the merged result to the tail. This greatly simply
the logic of handling odd sub lists case as shown in the above pseudo code.

\begin{Exercise}
\begin{itemize}
\item Implement the functional bottom-up merge sort by using folding.
\item Implement the iterative bottom-up merge sort only with array indexing. Don't use any library
supported tools, such as list, vector etc.
\end{itemize}
\end{Exercise}

\section{Parallelism}
\index{Parallel merge sort}
\index{Parallel quick sort}
We mentioned in the basic version of quick sort, that the two sub sequences can be sorted in
parallel after the divide phase finished. This strategy is also applicable for merge sort.
Actually, the parallel version quick sort and morege sort, do not only distribute
the recursive sub sequences sorting into two parallel processes, but divide the sequences into
$p$ sub sequences, where $p$ is the number of processors. Idealy, if we can achieve
sorting in $T'$ time with parallelism, which satisifies $O(n \lg n) = p T'$. We say it
is linear speed up, and the algorithm is parallel optimal.

However, a straightforward parallel extension to the sequential quick sort algorithm
which samples several pivots, divides $p$ sub sequences, and independently
sorts them in parallel, isn't optimal. The bottleneck exists in the
divide phase, which we can only achieve $O(n)$ time in average case.

The straightforward parallel extension to merge sort, on the other hand, block
at the merge phase. Both parallel merge sort and quick sort in practice need
good designs in order to achieve the optimal speed up. Actually, the divide and
conquer nature makes merge sort and quick sort relative easy for parallelisim.
Richard Cole found the $O(\lg n)$ parallel merge sort algorithm with $n$ processors
in 1986 in \cite{para-msort}.

Parallelism is a big and complex
topic which is out of the scope of this elementary book. Readers can refer to
\cite{para-msort} and \cite{para-qsort} for details.

\section{Short summary}
In this chapter, two popular divide and conquer sorting methods, quick sort and merge sort are introduced.
Both of them meet the upper performance limit of the comparison based sorting algorithms $O(n \lg n)$.
Sedgewick said that quick sort is the greatest algorithm invented in the 20th century. Almost
all programming environments adopt quick sort as the default sorting tool. As time goes on,
some environments, especially those manipulate abstract sequence which is dynamic and not based on
pure array switch to merge sort as the general purpose sorting tool\footnote{Actually, most of
them are kind of hybrid sort, balanced with insertion sort to achieve good performance when the
sequence is short}.

The reason for this interesting phenomena can be partly explained by the treatment in this chapter.
That quick sort performs perfectly in most cases, it needs fewer swapping than most other algorithms.
However, the quick sort algorithm is based on swapping, in purely functional settings, swapping isn't
the most efficient way due to the underlying data structure is singly linked-list, but not vectorized
array. Merge sort, on the other hand, is friendly in such environment, as it costs constant spaces,
and the performance can be ensured even in the worst case of quick sort, while the latter downgrade
to quadratic time. However, merge sort doesn't performs as well as quick sort in purely imperative
settings with arrays. It either needs extra spaces for merging, which is sometimes unreasonable, for
example in embedded system with limited memory, or causes many overhead swaps by in-place workaround.
In-place merging is till an active research area.

Although the title of this chapter is `quick sort vs. merge sort', it's not the case that one
algorithm has nothing to do with the other. Quick sort can be viewed as the optimized version of
tree sort as explained in this chapter. Similarly, merge sort can also be deduced from tree sort
as shown in \cite{sort-deriving}.

There are many ways to categorize sorting algorithms, such as in \cite{TAOCP}. One way is to
from the point of view of easy/hard partition, and easy/hard merge \cite{algo-fp}.

Quick sort, for example, is quite easy for merging, because all the elements in the sub
sequence before the pivot are no greater than any one after the pivot. The merging for
quick sort is actually trivial sequence concatenation.

Merge sort, on the other hand, is more complex in merging than quick sort. However, it's
quite easy to divide no matter what concrete divide method is taken:
simple divide at the middle point, even-odd splitting, nature splitting, or bottom-up
straight splitting. Compare to merge sort, it's more difficult for quick sort to
achieve a perfect dividing. We show that in theory, the worst case can't be completely
avoided, no matter what engineering practice is taken, median-of-three, random quick sort,
3-way partition etc.

We've shown some elementary sorting algorithms in this book till this chapter, including
insertion sort, tree sort, selection sort, heap sort, quick sort and merge sort. Sorting
is still a hot research area in computer science. At the time when this chapter is written,
people are challenged by the buzz word `big data', that the traditional convenient method
can't handle more and more huge data within reasonable time and resources.
Sorting a sequence of hundreds of Gigabytes becomes a routine in some fields.

\begin{Exercise}
  \begin{itemize}
    \item Design an algorithm to create binary search tree by using merge sort strategy.
  \end{itemize}
\end{Exercise}

\ifx\wholebook\relax\else

\begin{thebibliography}{99}

\bibitem{TAOCP}
Donald E. Knuth. ``The Art of Computer Programming, Volume 3: Sorting and Searching (2nd Edition)''. Addison-Wesley Professional; 2 edition (May 4, 1998) ISBN-10: 0201896850 ISBN-13: 978-0201896855

\bibitem{CLRS}
Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein.
``Introduction to Algorithms, Second Edition''. ISBN:0262032937. The MIT Press. 2001

\bibitem{qsort-impl}
Robert Sedgewick. ``Implementing quick sort programs''. Communication of ACM. Volume 21, Number 10. 1978. pp.847 - 857.

\bibitem{Bentley}
Jon Bentley. ``Programming pearls, Second Edition''. Addison-Wesley Professional; 1999. ISBN-13: 978-0201657883

\bibitem{3-way-part}
Jon Bentley, Douglas McIlroy. ``Engineering a sort function''. Software Practice and experience VOL. 23(11), 1249-1265 1993.

\bibitem{opt-qs}
Robert Sedgewick, Jon Bentley. ``Quicksort is optimal''. \url{http://www.cs.princeton.edu/~rs/talks/QuicksortIsOptimal.pdf}

\bibitem{fp-pearls}
Richard Bird. ``Pearls of functional algorithm design''. Cambridge University Press. 2010. ISBN, 1139490605, 9781139490603

\bibitem{algo-fp}
Fethi Rabhi, Guy Lapalme. ``Algorithms: a functional programming approach''. Second edition. Addison-Wesley, 1999. ISBN: 0201-59604-0

\bibitem{slpj}
Simon Peyton Jones. ``The Implementation of functional programming languages''. Prentice-Hall International, 1987. ISBN: 0-13-453333-X

\bibitem{msort-in-place}
Jyrki Katajainen, Tomi Pasanen, Jukka Teuhola. ``Practical in-place mergesort''. Nordic Journal of Computing, 1996.

\bibitem{okasaki-book}
Chris Okasaki. ``Purely Functional Data Structures''. Cambridge university press, (July 1, 1999), ISBN-13: 978-0521663502

\bibitem{sort-deriving}
Jos\`{e} Bacelar Almeida and Jorge Sousa Pinto. ``Deriving Sorting Algorithms''. Technical report, Data structures and Algorithms. 2008.

\bibitem{para-msort}
Cole, Richard (August 1988). ``Parallel merge sort''. SIAM J. Comput. 17 (4): 770-785. doi:10.1137/0217049. (August 1988)

\bibitem{para-qsort}
Powers, David M. W. ``Parallelized Quicksort and Radixsort with Optimal Speedup'', Proceedings of International Conference on Parallel Computing Technologies. Novosibirsk. 1991.

\bibitem{wiki-qs}
Wikipedia. ``Quicksort''. \url{https://en.wikipedia.org/wiki/Quicksort}

\bibitem{wiki-sweak-order}
Wikipedia. ``Strict weak order''. \url{https://en.wikipedia.org/wiki/Strict_weak_order}

\bibitem{wiki-total-order}
Wikipedia. ``Total order''. \url{http://en.wokipedia.org/wiki/Total_order}

\bibitem{wiki-harmonic}
Wikipedia. ``Harmonic series (mathematics)''. \url{https://en.wikipedia.org/wiki/Harmonic_series_(mathematics)}

\end{thebibliography}

\expandafter\enddocument
\fi
